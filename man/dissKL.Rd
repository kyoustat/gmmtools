% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/diss_KL.R
\name{dissKL}
\alias{dissKL}
\title{Kullback-Leibler Divergence}
\usage{
dissKL(gmmobj1, gmmobj2, method)
}
\arguments{
\item{gmmobj1}{first \code{gmm}-like object.}

\item{gmmobj2}{second \code{gmm}-like object.}

\item{method}{name of a method to be used, including \describe{
\item{"ut"}{unscented transform by Julier and Uhlmann (1996).}
\item{"gamerge"}{merge GMM into single Gaussian distribution.}
\item{"gasel"}{select KL of nearest pair of Gaussians.}
\item{"pog"}{product of Gaussians approximation.}
\item{"mb"}{matched bound approximation by Goldberger (2003).}
\item{"vlb"}{variational lower bound by Hershey and Olsen (2007).}
\item{"vub"}{variational upper bound by Hershey and Olsen (2007).}
}}
}
\value{
computed KL divergence value.
}
\description{
Kullback-Leibler divergence is an asymmetric measure of discrepancy for two 
measures
\deqn{D_{KL}(P,Q) = \int p(x) \log \frac{p(x)}{q(x)} dx} 
but for Gaussian mixture models, its analytical form does not exist. Here we 
implement some approximations for the quantity. Several methods are available 
which is described in the parameter section.
}
\examples{
\donttest{
# -------------------------------------------------------------
#              KL Divergence for Gaussian Mixtures
#
# Data 1 : use SMILEY data 'gensmiley()' function.
# Data 2 : SMILEY data is translated (+5)  and rotated (rot)
# Data 3 : SMILEY data is translated (+10) and rotated (rot).
# -------------------------------------------------------------
## GENERATE DATA
#  set up
ndata = 10
ntot  = 3*ndata
rot   = qr.Q(qr(matrix(rnorm(4),ncol=2)))

#  generate
list_data = list()
for (i in 1:ndata){
  list_data[[i]]           = (T4cluster::gensmiley(n=150, sd=0.1)$data)
  list_data[[i+ndata]]     = (T4cluster::gensmiley(n=150, sd=0.1)$data)\%*\%rot + 5
  list_data[[i+(2*ndata)]] = (T4cluster::gensmiley(n=150, sd=0.1)$data)\%*\%rot + 10
}

## FIT GMM MODELS WITH RANDOM K IN [4,10]
list_gmm = list()
for (i in 1:ntot){
  list_gmm[[i]] = gmm(list_data[[i]], k=sample(4:10,1))
}

## COMPUTE PAIRWISE KL DIVERGENCE
methods.all = c("ut","gamerge","gasel","pog","mb","vlb","vub")
methods.num = length(methods.all)
divergences = array(0,c(ntot,ntot,methods.num))
for (i in 1:ntot){
  gi = list_gmm[[i]]
  for (j in 1:ntot){
    gj = list_gmm[[j]]
    
    if (i!=j){
      for (k in 1:methods.num){
        divergences[i,j,k] <- dissKL(gi,gj,methods.all[k])
        divergences[j,i,k] <- divergences[i,j,k]
      }
    }
  }
}

## VISUALIZE
opar <- par(no.readonly=TRUE)
par(mfrow=c(2,4), pty="s")
plot(list_data[[1]], main="SMILEY data", pch=19)
for (k in 1:methods.num){
  tgt = divergences[,,k]
  image(tgt[,ntot:1], axes=FALSE, col=gray((0:128)/128),
        main=paste0("KL:",methods.all[k]))
}
par(opar)
}


}
\references{
\insertRef{julier_general_1996}{gmmtools}

\insertRef{goldberger_efficient_2003}{gmmtools}

\insertRef{hershey_approximating_2007-2}{gmmtools}
}
\concept{diss}
